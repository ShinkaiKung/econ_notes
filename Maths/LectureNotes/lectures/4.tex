% TeX root = ../Main.tex
% First argument to \section is the title that will go in the table of contents. Second argument is the title that will be printed on the page.

\section{Derivate with Direction}

\begin{definition}[Derivate with Direction]
    Let $f: D \subset  \mathbb{R}^{n} \to \mathbb{R}^{m}$. Let $\mathbf{x_0} \in int(D)$ and $\mathbf{v} \in \mathbb{R}^{n}$. The \textbf{derivative} of $f$ at point $\mathbf{x_0}$ (\textbf{with direction $\mathbf{v}$}) is:
    \[
        f'(\mathbf{x_0},\mathbf{v}) = \lim_{h \to 0} \frac{f(\mathbf{x_0}+h \mathbf{v}) - f(\mathbf{x_0})}{h}
    \]
    if the limit exists.
\end{definition}


\begin{example*}
    For $\mathbb{R}^{n} \to \mathbb{R}$ linear function $f$. We have $f'(\mathbf{x_0},\mathbf{v}) = f(\mathbf{v}).$
\end{example*}

\begin{theorem}[Mean Value Theorem]
    Let $f: \mathbb{R}^{n} \to \mathbb{R}$. Suppose $f'(\mathbf{x_0}+t \mathbf{v},\mathbf{v}) \exists \forall t \in [0,1]$. Then, $\exists \theta \in (0,1)$ such that \[
        f(\mathbf{x_0}+\mathbf{v}) - f(\mathbf{x_0}) = f'(\mathbf{x_0}+\theta \mathbf{v}, \mathbf{v}).
    \]
\end{theorem}

\section{Partial Derivative}

\begin{definition}[Partial Derivative]
    Let $f: \mathbb{R}^{n} \to \mathbb{R}$. The \textbf{partial derivative} of $f$ with $x_i$ at point $\mathbf{x_0}$ is:
    \[
        D_if(\mathbf{x_0}) = \frac{\partial f}{\partial x_i}(\mathbf{x_0}) = \lim_{h \to 0} \frac{f(\mathbf{x_0} + h \mathbf{e_i}) - f(\mathbf{x_0})}{h}
    \]
    if the limit exists.

    More generally, if $f: \mathbb{R}^{n} \to \mathbb{R}^{m} (f = (f_1, f_2, \dots,f_j,\dots,f_m)')$, the partial derivative of $f_j$ with respect to $x_i$ at $\mathbf{x_0}$ is:
    \[
        D_if_j(\mathbf{x_0}) = \frac{\partial f_j}{\partial x_i}(\mathbf{x_0}) = \lim_{h \to 0} \frac{f_j(\mathbf{x_0}_1, \dots, \mathbf{x_0}_i+h,\dots,\mathbf{x_0}_n)-f_j(\mathbf{x_0})}{h}
    \]
\end{definition}

\begin{remark*}
    Actually, the partial derivative is a derivative with direction $e_i$.
\end{remark*}
\section{Gradient}

\begin{definition}[Gradient]
    Let $f: \mathbb{R}^{n} \to \mathbb{R}$ and $\mathbf{x_0} \in \mathbb{R}^{n}$. Then the \textbf{gradient} is defined as: \[
        \nabla f(\mathbf{x_0}) = \Big(\frac{\partial f(\mathbf{x_0})}{\partial x_1}, \frac{\partial f(\mathbf{x_0})}{\partial x_2},\dots,\frac{\partial f(\mathbf{x_0})}{\partial x_n}\Big)
    \]
\end{definition}
\begin{remark*}
    The gradient is a collection of partial derivatives and described all the change rate at all axis' directions.
\end{remark*}

\section{Directional Derivative}

\begin{definition}[Directional Derivative]
    Let $f: \mathbb{R}^{n}\to \mathbb{R}^{m}, \mathbf{x_0}\in \mathbb{R}^{n}$ and $\mathbf{v}\in \mathbb{R}^{n}$ \underline{with $||\mathbf{v}||$}. Then the \textbf{directional derivative} of $f$ at $\mathbf{x_0}$ with direction $\mathbf{v}$ is given by: \[
        D_\mathbf{v}f(\mathbf{x_0}) =f'(\mathbf{x_0},\mathbf{v}) = \lim_{h \to 0} \frac{f(\mathbf{x_0+h \mathbf{v}})-f(\mathbf{x_0})}{h}
    \]
    where $h \in \mathbb{R}$ is a scalar, assuming the limit exists.
\end{definition}

\begin{remark*}
    Directional Derivative is actually a specific situation of derivative with direction when $||\mathbf{v}||=1$.

    Partial Derivate can be also seen as a special case of Directional Derivative when $\mathbf{v} = \mathbf{e_i}$.
\end{remark*}

\begin{remark*}
    Partial derivatives give the slope of the function when you move in directions parallel to the coordinate axis, i.e., when you keep all other coordinates constant with the exception of one variable.

    The notion of a directional derivative, however, allows us to compute the instantaneous rate of change of a function, i.e., when all of the variables move.
\end{remark*}

\begin{proposition}
    Let $f: \mathbb{R}^{n} \to \mathbb{R}^{m}, f_i: \mathbb{R}^{n} \to \mathbb{R}$ for $i=1,\dots,m, \mathbf{x_0} \in \mathbb{R}^{n}, \mathbf{v} \in \mathbb{R}^{n}$, then \[
        \exists f'(\mathbf{x_0}, \mathbf{v}) \iff \forall i = 1,\dots,m \quad f'_i(\mathbf{x_0},\mathbf{v})
    \]
\end{proposition}

\begin{remark*}
    Suppose $f: D \subseteq \mathbb{R}^{n} \to \mathbb{R}, \mathbf{x_0} \in \mathbb{R}^{n}$, and $\exists f'(\mathbf{x_0},\mathbf{v})$ for all $\mathbf{v} \in \mathbb{R}^{n}$. It could be the case that $f$ is not continuous at $\mathbf{x_0}$.

    Explain: all the derivative with directions are getting close to $\mathbf{x_0}$ linearly. But it could be found a non-linear direction which crosses many $\mathbf{v_i}$ make the limit varies.

    Example:
    $f(x,y) = \left\{\begin{array}{l}
            \frac{xy ^{2}}{x ^{2} + y ^{4}} \\
            0                               \\
        \end{array}\right.$
\end{remark*}


\section{Differentiability}

\begin{definition}
    Let $f: D \subseteq \mathbb{R}^{n} \to \mathbb{R}^{m}$. $f$ is \textbf{differentiable} at $\mathbf{x_0} \iff \forall \mathbf{h} \in \mathbb{R}^{n}, \exists Df(\mathbf{x_0})$ such that the following limit exists \[
        \lim_{h \to 0} \frac{||f(\mathbf{x_0}+\mathbf{h})-f(\mathbf{x_0})-Df(\mathbf{x_0})\mathbf{h}||}{||\mathbf{h}||} = 0
    \]
    where $Df(\mathbf{x_0})$ is called Jacobian Matrix.
    \begin{remark*}
        $f(\mathbf{x_0}+\mathbf{h})-f(\mathbf{x_0}) = Df(\mathbf{x_0})\mathbf{h}+g(\mathbf{h})$ with $\lim_{h \to 0} \frac{g(\mathbf{h})}{\mathbf{h}} = 0$. It's a linear approximation of $f.$
    \end{remark*}
\end{definition}

\begin{proposition}
    Let $f: D \subseteq \mathbb{R}^{n} \to \mathbb{R}^{m}$, and $\mathbf{x_0} \in \mathbb{R}^{n}$.
    $f$ is differentiable at $\mathbf{x_0} \iff f_i$ is differentiable at $\mathbf{x_0}\, \forall i = 1,\dots,m$.
\end{proposition}

\begin{proposition}
    If $f: \mathbb{R}^{n} \to \mathbb{R}$ is differentiable at $\mathbf{x_0}\in \mathbb{R}^{n}$, with differential $L_\mathbf{x_0}$, then $\forall \mathbf{v} \in \mathbb{R}^{n}, \exists f'(\mathbf{x_0}, \mathbf{v})$, in addition \[
        L_\mathbf{x_0}(\mathbf{v}) = f'(\mathbf{x_0},\mathbf{v})= \nabla f(\mathbf{x_0}) \cdot \mathbf{v}.
    \]
    , where $L_\mathbf{x_0}$ represents the linear part of the approximation $f(\mathbf{x_0}+\mathbf{h})-f(\mathbf{x_0}) = Df(\mathbf{x_0})\mathbf{h}+g(\mathbf{h})$. So we have $L_\mathbf{x_0}(\mathbf{v} + \mathbf{h})= L_\mathbf{x_0}(\mathbf{v})+L_\mathbf{x_0}(\mathbf{h})$ and $L_\mathbf{x_0}(\lambda \mathbf{v})=\lambda L_\mathbf{x_0}(\mathbf{v})$.
\end{proposition}

\begin{proof}
    If $\mathbf{v} = 0$, it is trivial that $L_{\mathbf{x_0}}(\mathbf{v}) = 0$ and $f'(\mathbf{x_0},\mathbf{v}) = 0$.

    If $\mathbf{v} \neq 0$, $f(\mathbf{x_0} + \mathbf{w}) - f(\mathbf{x_0})= L_\mathbf{x_0}(\mathbf{w}) + g(\mathbf{w}), \lim_{\mathbf{w} \to \mathbf{0}} \frac{g(\mathbf{w})}{||\mathbf{w}||} = 0$. Take $\mathbf{w} = h \mathbf{v} (h \neq 0)$, \begin{align*}
        f'(\mathbf{x_0},\mathbf{v}) = \lim_{h \to 0} \frac{f(\mathbf{x_0}+h \mathbf{v}) - f(\mathbf{x_0})}{h}
         & = \lim_{h \to 0} \frac{L_{x_0}(h \mathbf{v}) + g(h \mathbf{v})}{h}                                                                           \\
         & =\lim_{h \to 0} \frac{h L_\mathbf{x_0}(\mathbf{v})}{h} + \lim_{h \to 0} \frac{g(h \mathbf{v})}{||h \mathbf{v}||} \frac{|h|||\mathbf{v}||}{h} \\
         & = L_\mathbf{x_0}(\mathbf{v}) + ||\mathbf{v}|| \lim_{h \to 0} \frac{|h|}{h}\frac{g(h \mathbf{v})}{||h \mathbf{v}||}                           \\
         & = L_\mathbf{x_0}(\mathbf{v})
    \end{align*}
    Using $\mathbf{v} = \sum_{i=1}^{n} v_i \mathbf{e}_i$, \begin{align*}
        L_\mathbf{x_0}(\mathbf{v}) & = L_\mathbf{x_0}\Big(\sum_{i=1}^{n} v_i \mathbf{e}_i \Big) = \sum_{i=1}^{n} v_i L_\mathbf{x_0}(\mathbf{e}_i) = \sum_{i=1}^{n} v_i f'(\mathbf{x_0}, \mathbf{e}_i) = \sum_{i=1}^{n} v_i \frac{\partial f(\mathbf{x_0})}{\partial x_i} = \nabla f(\mathbf{x_0}) \cdot  \mathbf{v}
    \end{align*}
\end{proof}

\begin{remark*}
    \(D_{\mathbf{v}}f(\mathbf{x}_0) = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}\). This means that, for a real-valued function, we can write the directional derivative as the inner product of the gradient and the direction.
\end{remark*}


\begin{proposition}
    Let \(f:\mathbb{R}^{n}\longrightarrow \mathbb{R}^{m}\), and \(\mathbf{x}_{0}\in \mathbb{R}^{n}\). If \(f\) is differentiable at \(\mathbf{x}_{0}\) , then it is continuous at \(\mathbf{x}_{0}\)
\end{proposition}

\begin{remark*} Summary:

    Differentiable $\Longrightarrow$ Continuous.

    Differentiable $\Longrightarrow$ Partial Derivates exist.

    Continuous $\nLeftrightarrow$ Partial Derivates exist.
\end{remark*}



\section{Jacobian Matrix and Chain Rule}

\begin{definition}
    The matrix of partial derivatives of all the $f_i$ with respect to all the $x_i$ evaluated at $\mathbf{x_0}$ is called the \textbf{Jacobian} of $\mathbf{f}$ at $\mathbf{x_0}$: \[
        \mathbf{Jf(x_0)} = \begin{pmatrix}
            \nabla f_1(\mathbf{x_0})' \\ \dots \\ \nabla f_m(\mathbf{x_0})'
        \end{pmatrix} = \begin{pmatrix}
            \frac{\partial f_1(\mathbf{x_0})}{\partial x_1} & \dots  & \frac{\partial f_1(\mathbf{x_0})}{\partial x_n} \\
            \vdots                                          & \ddots & \vdots                                          \\
            \frac{\partial f_m(\mathbf{x_0})}{\partial x_1} & \dots  & \frac{\partial f_m(\mathbf{x_0})}{\partial x_n}
        \end{pmatrix}
    \]
\end{definition}

\begin{remark*}
    If $m=n$, then the Jacobian matrix is a square matrix. We can calculate the determinant and inverse of it.
\end{remark*}

\begin{proposition}
    Let \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) and \(g: \mathbb{R}^{m} \rightarrow \mathbb{R}^{p}\) be differentiable. Let \(f\) be continuously differentiable at some set \(U\), and \(g\) be continuously differentiable on \(f(U)\). Then, \(h: \mathbb{R}^{n} \rightarrow \mathbb{R}^{p}\), \(h(\mathbf{x}) = g[f(\mathbf{x})]\) is continuously differentiable on \(U\), with:
    \[
        D h(\mathbf{x}) = D g[f(\mathbf{x})]D f(\mathbf{x}).
    \]
\end{proposition}

\begin{remark*}
    About notation:

    If \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) , we will denote
    \(D f(x)\mathbf{v}\) as \(\mathbf{J}f(\mathbf{x})\mathbf{v}\) while,

    if \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) , we will denote
    \(D f(x)\mathbf{v}\) as \(\nabla f(\mathbf{x}) \cdot \mathbf{v}\) and,
    finally,

    if \(f: \mathbb{R} \rightarrow \mathbb{R}^{m}\) , we will denote
    \(D f(x)v\) as \(f'(x)v\).
\end{remark*}


\section{Additional Results}

\begin{proposition} Let \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}, \mathbf{v} \in \mathbb{R}^{n}\), and \(||\mathbf{v}|| = 1\). Then
    \[
        \max_{\mathbf{v}}f^{\prime}(\mathbf{x}_{0},\mathbf{v}) = ||\nabla f(\mathbf{x}_{0})|| \quad \text{when } \mathbf{v} = \frac{\nabla f(\mathbf{x}_{0})}{||\nabla f(\mathbf{x}_{0})||},
    \]
    \[
        \min_{\mathbf{v}}f^{\prime}(\mathbf{x}_{0},\mathbf{v}) = -||\nabla f(\mathbf{x}_{0})|| \quad \text{when } \mathbf{v} = -\frac{\nabla f(\mathbf{x}_{0})}{||\nabla f(\mathbf{x}_{0})||},
    \]

    and \(\mathbf{v} \perp \nabla f(\mathbf{x}_{0}) \Rightarrow f^{\prime}(\mathbf{x}_{0}, \mathbf{v}) = 0\).
\end{proposition}

\begin{proposition}[Sufficient Condition of Differentiability]
    For function $f$, if all partial derivatives exist in $B(\mathbf{x_0},r)$, and they are continuous at $\mathbf{x_0}$, $\Longrightarrow f$ is differentiable at $\mathbf{x_0}$
\end{proposition}

\section{Level Sets (and the gradient)}


\begin{definition}
    $C_\alpha = \{\mathbf{x} \in \mathbb{R}^{n}: f(\mathbf{x}) = \alpha\}$ is a \textbf{level set} of function $f: \mathbb{R}^{n} \to \mathbb{R}$ at a given level $\alpha$.
\end{definition}

If $\mathbf{x_0}$ lies on the level set, the \textit{tangent hyperplane} to the level set is the set of all $\mathbf{x}$ that satisfy: \[
    \frac{\partial f(\mathbf{x_0})}{\partial x_1}(x_1-x_1 ^{0}) + \dots+ \frac{\partial f(\mathbf{x_0})}{\partial x_n}(x_n-x_n ^{0}) = \mathbf{0}.
\]
, which can be written as \[
    \nabla f(\mathbf{x_0}) \cdot (\mathbf{x - x_0}) = \mathbf{0}.
\]

\begin{proposition}
    $\nabla f(\mathbf{x_0})$ is orthogonal to the level set of $f$ that contains $\mathbf{x_0}$.
\end{proposition}

\begin{proof}
    To prove $\nabla f(x_0)$ is orthogonal to an arbitrary curve which passes $\mathbf{x_0}$ in level set.

    Let $f(\mathbf{x_0}) = \alpha$ and $C_\alpha = \{\mathbf{x} \in \mathbb{R}^{n}: f(\mathbf{x}) = \alpha\}$. Let $\Gamma$ be a curve within the level set of $f$ at $\alpha$, that is $\Gamma \subseteq C_\alpha$ and passed through $\mathbf{x_0}$. \[
        \mathbf{x} \in \Gamma \, \text{if } \begin{pmatrix}
            x_1 \\ \dots \\ x_n
        \end{pmatrix} = \begin{pmatrix}
            \phi_1(t) \\ \dots \\ \phi_n(t)
        \end{pmatrix} := \boldsymbol{\phi}(\mathbf{x}), \, \text{with } t \in \mathbb{R} \text{ so that } \mathbf{x_0} = \boldsymbol{\phi}(t)
    \]

    Let $g(t) = (f \circ \phi )(t) = f[\phi (t)]$, since $g(t) = a$, then \[
        g'(t) = 0 = \nabla f[\phi (t)] \cdot \phi' (t).
    \]
    In particular, when $t=t_0$, \[
        \nabla f(\mathbf{x_0}) \cdot \phi'(t_0) = 0.
    \]
\end{proof}

\section{Higher-order Derivatives}

Suppose \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\). Then \(f' : \mathbb{R}^{n} \rightarrow \mathcal{M}_{mn}\). That is, there are \(nm\) first- order partial derivatives.

Each of these has \(n\) partial derivatives, so \(f\) has \(n^{2}m\) second order partial derivatives, written \(\partial^{2}f_{k} / \partial x_{i}\partial x_{j}\), for \(k = 1, \ldots , m\) and \(i, j = 1, \ldots , n\).

We just need to stack all columns of \(f'\) and just treat it as a function from \(\mathbb{R}^{n}\) to \(\mathbb{R}^{mn}\).

If we keep on going, we will find that the vector that corresponds to \(f^{(k)}(\cdot)\) will have size \(n^{k}m\) , which grows quickly with \(k\) .


\subsection{Second-order Partial Derivatives}

\begin{definition}
    Let $f: \mathbb{R}^{n} \to \mathbb{R}$. The \textbf{second partial derivative} with respect to $x_i$ and $x_j$ at point $\mathbf{x_0}$ is \[
        D_{ij}f(\mathbf{x_0}) = \frac{\partial ^{2}f(\mathbf{x_0})}{\partial x_i \partial x_j} = \lim_{h \to 0} \frac{f'(\mathbf{x_0}+h \mathbf{e_j}, \mathbf{e_i}) - f_i(\mathbf{x_0})}{h},
    \] assuming such a limit exists.
\end{definition}


\begin{definition}
    Let \(f: D \subset \mathbb{R}^{n} \rightarrow \mathbb{R}\). We say that \(f\) is twice differentiable at \(\mathbf{x}_{0}\) if it is differentiable in an open set around \(\mathbf{x}_{0}\) (so that \(f'\) is defined in an open set around \(\mathbf{x}_{0}\)) and \(\nabla f\) is differentiable at \(\mathbf{x}_{0}\).
\end{definition}

\begin{definition}
    The \textbf{Hessian} matrix is:
    \[
        \mathbf{H}f(\mathbf{x}) = \begin{pmatrix}
            \frac{\partial ^{2}f(\mathbf{x})}{\partial x_1 \partial x_1} & \frac{\partial ^{2}f(\mathbf{x})}{\partial x_1 \partial x_2} & \dots & \frac{\partial ^{2}f(\mathbf{x})}{\partial x_1 \partial x_n} \\
            \frac{\partial ^{2}f(\mathbf{x})}{\partial x_2 \partial x_1} & \ddots                                                       &       & \frac{\partial ^{2}f(\mathbf{x})}{\partial x_2 \partial x_n} \\
            \vdots                                                       &                                                              &       & \vdots                                                       \\
            \frac{\partial ^{2}f(\mathbf{x})}{\partial x_n \partial x_1} & \dots                                                        &       & \frac{\partial ^{2}f(\mathbf{x})}{\partial x_n \partial x_n}
        \end{pmatrix}
    \]
\end{definition}

\begin{theorem}[Schwarz's - or Clairaut's- Theorem]
    If \(f\) is \(C^{2}\), then:
    \[
        \frac{\partial^{2}f(x)}{\partial x_{j}\partial x_{i}} = \frac{\partial^{2}f(x)}{\partial x_{i}\partial x_{j}}.
    \]
    It means Hessian matrix is symmetric.
\end{theorem}

\section{Taylor Formula}

\begin{theorem}
    Let \(f:D\subseteq \mathbb{R}^{n}\to \mathbb{R}\) : \(f\in C^{q}\) in \(D\) (i.e.~it has continuous derivatives up to order \(q\)). Let \(\mathbf{x},\mathbf{x_0}:[\mathbf{x_0},\mathbf{x}]\subseteq D\) and define \(\mathbf{v} = \mathbf{x} - \mathbf{x_0}\) Then, \(\exists \theta \in (0,1)\) :

    \[
        \begin{array}{rcl}{f(\mathbf{x})}&{=}&{f(\mathbf{x_0})+\sum_iD_if(\mathbf{x_0})(x_i-x_{0i})}\\{}&{}&{+\frac{1}{2}\sum_{i,j}D_{ij}f(\mathbf{x_0})(x_i-x_{0i})(x_j-x_{0j})+\cdots}\\{}&{}&{+\frac{1}{(q-1)!}\sum_{i_1\dots i_{q-1}}D_{i_1}\dots_{i_{q-1}}f(\mathbf{x_0})(x_{i_1}-x_{0i_1})\cdots(x_{i_{q-1}}-x_{0i_{q-1}})}\\{}&{}&{+\frac{1}{q!}\sum_{i_1\dots i_q}D_{i_1}\dots_{i_q}f(\mathbf{x_0}+\theta\mathbf{v})(x_{i_1}-x_{0i_1})\cdots(x_{i_q}-x_{0i_q}).}\end{array}
    \]
\end{theorem}

\begin{corollary}
    If $q = 2$, the first-order Taylor approximation is \[
        f(\mathbf{x}) = f(\mathbf{x_0}) + \nabla f(\mathbf{x_0}) \cdot (\mathbf{x - x_0}) + R_2(\mathbf{x})
    \]
\end{corollary}

\begin{corollary}
    If $q = 3$, the first-order Taylor approximation is \[
        f(\mathbf{x}) = f(\mathbf{x_0}) + \nabla f(\mathbf{x_0}) \cdot (\mathbf{x - x_0}) + \frac{1}{2}(\mathbf{x - x_0})'\mathbf{H}f(\mathbf{x_0})(\mathbf{x-x_0}) + R_3(\mathbf{x})
    \]
\end{corollary}


\section{Multiple Integrals}

\begin{theorem}[Fubini's Theorem]
    Suppose \[
        \int\int_{X \times Y} |f(x,y)|d(x,y) < \infty.
    \]
    Then,
    \[
        \int\int_{X \times Y} f(x,y)d(x,y) = \int_{Y}\left(\int_{X}f(x,y)dx\right)dy = \int_{X}\left(\int_{Y}f(x,y)dy\right)dx.
    \]
\end{theorem}

\section{Injectivity, Surjectivity, and Bijectivity}

Consider a mapping $f: A \to B$, an element of the codomain $\mathbf{y} \in B$, and the inverse image of $\mathbf{y}, f ^{-1}(\mathbf{y})$.

\begin{definition}
    A mapping $f: A \to B$ is \textbf{injective} or \textbf{one-to-one} $\iff$ \[
        \forall \mathbf{x},\mathbf{x'} \in A, f(\mathbf{x}) = f(\mathbf{x'}) \Longrightarrow \mathbf{x} = \mathbf{x'}.
    \]
\end{definition}


\begin{definition}
    A mapping $f: A \to B$ is \textbf{surjective} or \textbf{onto} $\iff$ \[
        \forall \mathbf{y} \in B, \exists \mathbf{x} \in A \Longrightarrow \mathbf{y} = f(\mathbf{x}).
    \]
\end{definition}

\begin{definition}
    A mapping $f: A \to B$ is \textbf{bijective} $iff$ it is both \textbf{injective} and \textbf{surjective}.
\end{definition}

\section{Inverse Function}

\begin{definition}
    The \textbf{identity mapping} from $A$ to $A$ is the mapping $Id_X: A \to A, Id_X(\mathbf{x}) = \mathbf{x}$.
\end{definition}

\begin{definition}
    $f$ has an \textbf{inverse} if there exists a mapping $g: B \to A$ such that $f \circ g = Id_Y$ and $g \circ f = Id_X$.
\end{definition}

\begin{proposition}
    $f$ is bijective $\iff$ $f$ has an inverse.
\end{proposition}

\begin{theorem}[Inverse Function Theorem]
    Let $f: A \subset \mathbb{R}^{n} \to B \subset \mathbb{R}^{n}$, with $A$ open. Let $\mathbf{a} \in A$ and $\mathbf{b} = f(\mathbf{a})$. Assume:
    \begin{enumerate}
        \item $f$ has all its first partial derivatives, and they are continuous in $A$; and
        \item $det[\mathbf{J}f(\mathbf{a})] \neq 0$.
    \end{enumerate}
    Then, \begin{enumerate}
        \item \textbf{the inverse function $\varphi: W \to V$ exists} (where $V, W$ are open, $\mathbf{a} \in V, \mathbf{b} \in W, \varphi (\mathbf{y}) = \mathbf{x}$),
        \item $\varphi$ is differentiable, and its differential is the inverse of $\mathbf{J}f(a)^{-1}$.
    \end{enumerate}
    In addition, if $f$ is $C ^{k}$, then $\varphi$ is also $C ^{k}$ in $W$.
\end{theorem}

\begin{remark*}
    $\forall \mathbf{x} \in A, \det (\mathbf{J}f(\mathbf{x})) \neq 0 \nRightarrow$ $f$ injective. But it could have inverse function at some local neighbor.

    \begin{example*}
        $f: A \subset \mathbb{R}^{2} \to \mathbb{R}$ with $A = \{(x_1,x_2)\in \mathbb{R}^{2}: x_1 ^{2} + x_2 ^{2} \geq 1\}$ and $f(x_1,x_2) = (x_1 ^{2} - x_2 ^{2}, x_1x_2)$.

        $\forall \mathbf{x} \in A, \det (\mathbf{J}f(\mathbf{x})) \neq 0$, but $f(1,1) = f(-1,-1)$.
    \end{example*}
\end{remark*}



\subsection{"Heuristic" argument}
\begin{align*}
     & f(\mathbf{x}) = f(\mathbf{a}) + \mathbf{J}f(\mathbf{a})(\mathbf{x} - \mathbf{a}) + R_1                                                                                                            \\
     & \mathbf{y} = f(\mathbf{a}) + \mathbf{J}f(\mathbf{a})(\mathbf{x} - \mathbf{a}) \Longrightarrow \mathbf{J}f(\mathbf{a})\mathbf{x} = \mathbf{y} - f(\mathbf{a}) + \mathbf{J}f(\mathbf{a})\mathbf{a}. \\
     & \mathbf{x} = [\mathbf{J}f(\mathbf{a})]^{-1}[\mathbf{y} - f(\mathbf{a})+\mathbf{J}f(\mathbf{a})\mathbf{a}] = \mathbf{a} + [\mathbf{J}f(\mathbf{a})]^{-1}[\mathbf{y}-f(\mathbf{a})]
\end{align*}
That's why we need $det[\mathbf{J}f(\mathbf{a})] \neq 0$.



\section{Implicit Function Theorem}

\begin{theorem}[Implicit Function Theorem]
    Let $f: \mathbb{R}^{n} \times \mathbb{R}^{m} \to \mathbb{R}^{m}, (\mathbf{x}, \mathbf{y}) \to f(\mathbf{x}, \mathbf{y})$, and let $(\mathbf{a}, \mathbf{b}) \in \mathbb{R}^{n} \times \mathbb{R}^{m}$. Assume:
    \begin{enumerate}
        \item $f(\mathbf{a}, \mathbf{b}) = \mathbf{0}$,
        \item $f$ has all its partial derivatives continuous in an open set the contains $(\mathbf{a}, \mathbf{b})$, and
        \item $\left|\frac{\partial (f_1,\dots,f_m)'}{\partial (y_1,\dots,y_m)}\right|_{(\mathbf{x,y}) = (\mathbf{a,b})} \neq 0$.
    \end{enumerate}
    Then, \begin{enumerate}
        \item \textbf{the implicit function $g: A \to B$ exists} (where $A,B$ are open, $A \subset \mathbb{R}^{n}, \mathbf{a} \in A, B \subset \mathbb{R}^{m}, \mathbf{b} \in B$, and $\forall \mathbf{x} \in A, f(\mathbf{x}, g(\mathbf{x})) = \mathbf{0}$).
        \item $g$ is continuously differentiable for all $\mathbf{x} \in A$.
    \end{enumerate}
\end{theorem}

\begin{proof}[Proof of Implicit Function Theorem]
    Define a mapping $F: \mathbb{R}^{n} \times \mathbb{R}^{m} \to \mathbb{R}^{n} \times \mathbb{R}^{m}$ such that \[
        (\mathbf{x}, \mathbf{y}) \mapsto F(\mathbf{x}, \mathbf{y}) := (\mathbf{x}, f(\mathbf{x,y}))
    \]
    That's check the two requirements of \textbf{Inverse Function Theorem} for $F$:

    1) F has all its first order partial derivatives, and they are continuous on an open set contains $(\mathbf{a, b})$. It is trivial.

    2)$\det (\mathbf{J}F) = \big|\frac{\partial (F_1,\dots,F_n,F_{n+1},\dots,F_{n+m})}{\partial (x_1,\dots,x_n,y_1,\dots,y_m)}\big|_{\mathbf{x=a,y=b}} = \begin{vmatrix}
            \boldsymbol{I_n}                                           & \boldsymbol{0}                                             \\
            \frac{\partial (f_1,\dots,f_m)'}{\partial (x_1,\dots,x_n)} & \frac{\partial (f_1,\dots,f_m)'}{\partial (y_1,\dots,y_m)}
        \end{vmatrix} = \left|\frac{\partial (f_1,\dots,f_m)'}{\partial (y_1,\dots,y_m)}\right| \neq 0$

    Specifically, there exists open $W \subseteq \mathbb{R}^{n} \times \mathbb{R}^{n}$ with $\mathbf{b} \in W$, such that $F(\mathbf{a,b}) \in W$,
    \[
        F(\mathbf{a}, \mathbf{b}) = (\mathbf{a}, f(\mathbf{a,b})) = (\mathbf{a, 0})
    \]
    Then there exists an open set in $\mathbb{R}^{n} \times \mathbb{R}^{n}$, say $A \times B \subseteq \mathbb{R}^{n} \times \mathbb{R}^{m}$ with $\mathbf{a} \in A, \mathbf{b} \in B$, such that:
    i) there exists $h: W \to A \times B$, with $h$ being the inverse function of $F$, and
    ii) $h$ is differentiable.

    Specifically, $h(\mathbf{x',y'}) = (\mathbf{x,y})$ with $F(\mathbf{x,y}) = (\mathbf{x',y'}) = (\mathbf{x, f(\mathbf{x,y})})$, which implies \[
        \left\{\begin{array}{l}
            \mathbf{x} = \mathbf{x'}      \\
            f(\mathbf{x,y}) = \mathbf{y'} \\
        \end{array}\right.
    \]
    Therefore, $h$ has to have the following form: ($f(\mathbf{x',y}) = \mathbf{y'} \Rightarrow  \mathbf{y} = k(\mathbf{x',y'})$ - inverse function) \[
        h(\mathbf{x',y'}) = (\mathbf{x'},k(\mathbf{x',y'})), \, \text{with $k$ differentiable}.
    \]

    Consider now the function $\pi : \mathbb{R}^{n} \times \mathbb{R}^{m} \to \mathbb{R}^{m}$ such that $(\mathbf{x,y}) \mapsto \pi (\mathbf{x,y}) := \mathbf{y}$. Then \[
        (\pi \circ F)(\mathbf{x,y}) = \pi [F(\mathbf{x,y})] = \pi [\mathbf{x, f(\mathbf{x,y})}] = f(\mathbf{x,y}).
    \]
    Then, for function $f[\mathbf{x}, k(\mathbf{x,y})] $ we have \[
        f[\mathbf{x}, k(\mathbf{x,y})] = f[h(\mathbf{x,y})] = (\pi \circ F)[h(\mathbf{x,y})] = \pi [(F \circ h)(\mathbf{x,y})] = \pi (\mathbf{x,y}) = \mathbf{y}
    \]

    Take $g(\mathbf{x}) = k(\mathbf{x, 0})$, then $\forall \mathbf{x} \in A$:

    1) $f(\mathbf{x},g(\mathbf{x})) = \mathbf{0}$, and

    2) $g$ is differentiable.
\end{proof}


\section{Convex Set}

\begin{definition}[Convex Set]
    Let $C \subseteq \mathbb{R}^{n}$. Then, $C$ is a \textbf{convex set} $\iff \forall \mathbf{x,y} \in C, \forall \lambda \in [0,1], \lambda \mathbf{x} + (1-\lambda)\mathbf{y} \in C$.
\end{definition}

\begin{remark*}
    $\emptyset $ and $\mathbb{R}^{n}$ are convex.
\end{remark*}

\begin{proposition}
    Let $C_i \subset \mathbb{R}^{n}$ be convex $\forall i \in \boldsymbol{I} \Longrightarrow \bigcap_{i \in I}^{}C_i$ is convex.
\end{proposition}

\begin{remark*}
    The union of convex sets is usually not convex.
\end{remark*}


\begin{definition}[Convex Linear Combination]
    Let $\mathbf{x_1, x_2, \dots, x_m} \in \mathbb{R}^{n}$ and $\lambda_1,\lambda_2,\dots,\lambda_m \in [0,1]$. Then, a \textbf{convex linear combination} of $\mathbf{x_1, x_2, \dots, x_m}$ is a vector \[
        \lambda_1 \mathbf{x_1} + \lambda_2 \mathbf{x_2} + \dots + \lambda_m \mathbf{x_m} \text{ with } \sum_{i=1}^{m} \lambda_i = 1.
    \]
\end{definition}

\begin{proposition}
    Let $C \subset \mathbb{R}^{n}$ be convex. Then any convex linear combination of the element in $C$ is in $C$.
\end{proposition}

\begin{definition}[Convex Envelope]
    Let $A = \{\mathbf{x_1, x_2, \dots,x_m}\} \subset \mathbb{R}^{n} $. The set of convex linear combination of $\mathbf{x_1, x_2, \dots,x_m}, i.e., Ec(A) = \{\sum_{i=1}^{m} \lambda_i \mathbf{x_i} : \lambda_i \in [0,1], \sum_{i=1}^{m} \lambda_i = 1, \mathbf{x_1, x_2, \dots, x_m} \in A \}$, is called \textbf{convex envelope} of $A$.
\end{definition}

\begin{remark*}
    Convex envelope is the smallest convex set which contains set $A$.
\end{remark*}

\section{Separating Hyperplane}

\begin{definition}[Hyperplane]
    A subset \(H\) of \(\mathbb{R}^{n}\) is a \textbf{hyperplane} if
    there exists \(\mathbf{a} \neq \mathbf{0} \in \mathbb{R}^{n}\) and
    \(c \in \mathbb{R}\) such that we can define the following set:
    \[
        \begin{array}{rl} & H_{a} = \{\mathbf{x} \in \mathbb{R}^{n}: a_{1} x_{1} + a_{2} x_{2} + \ldots + a_{n} x_{n} = c \} \\ & \quad = \{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{a}^{\prime} \mathbf{x} = c \} . \end{array}
    \]
\end{definition}

\begin{definition}
    We can also define closed and open half- spaces:

    Closed: \(\overline{H_{a}^{+}} \equiv \left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{a}^{\prime} \mathbf{x} \geq c \right\}\) and \(\overline{H_{a}^{- }} \equiv \left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{a}^{\prime} \mathbf{x} \leq c \right\}\).

    Open: \(H_{a}^{+} \equiv \left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{a}^{\prime} \mathbf{x} > c \right\}\) and \(H_{a}^{- } \equiv \left\{\mathbf{x} \in \mathbb{R}^{n}: \mathbf{a}^{\prime} \mathbf{x}< c \right\}\)
    .
\end{definition}

\begin{remark*}
    Hyperplanes and half-spaces are convex.
\end{remark*}

\begin{theorem}[Strict Separation Theorem]
    Let \(A \subset \mathbb{R}^{n}, B \subset \mathbb{R}^{n}\). If \(A, B\) are convex, \textbf{closed}, disjoint, non-empty, with \textbf{at least one of them bounded}, then \(\exists H_{a}^{+}\) and \(H_{a}^{- }\) such that \(A \subset H_{a}^{+}\) and \(B \subset H_{a}^{- }\).
\end{theorem}


\begin{theorem}[Weak Separation Theorem]
    Let \(A \subset \mathbb{R}^{n}, B \subset \mathbb{R}^{n}\). If \(A, B\) are convex, disjoint, and non-empty, then \(\exists \overline{H_{a}^{+}}\) and \(H_{a}^{- }\) such that \(A \subset \overline{H_{a}^{+}}\) and \(B \subset \overline{H_{a}^{- }}\).
\end{theorem}

\section{Convex and Concave Functions}

\begin{definition}[Convex Function]
    A convex function is defined as a function whose \textbf{area above the curve}: \(\{(\mathbf{x}, \mathbf{y}): \mathbf{y} \geq f(\mathbf{x})\}\) (called the epigraph or the hypergraph) is a \textbf{convex set}.
\end{definition}

\begin{definition}[Concave Function]
    A concave function is defined as a function whose \textbf{area below the curve}: \(\{(\mathbf{x}, \mathbf{y}): \mathbf{y} \leq f(\mathbf{x})\}\) (called the subgraph or the hypergraph) is a \textbf{convex set}.
\end{definition}

\textbf{Equivalent definitions:}

\begin{definition}[Convex Function]
    \(f\) is a convex function if \(C\) is a convex subset of \(\mathbb{R}^{n}\) and:
    \[
        \mathbf{y} \in C, \forall \lambda \in [0,1], f(\lambda \mathbf{x} + (1 - \lambda)\mathbf{y}) \leq \lambda f(\mathbf{x}) + (1 - \lambda)f(\mathbf{y})
    \]
\end{definition}

\begin{definition}[Concave Function]
    \(f\) is a concave function if \(C\) is a convex subset of \(\mathbb{R}^{n}\) and:
    \[
        \mathbf{y} \in C, \forall \lambda \in [0,1], f(\lambda \mathbf{x} + (1 - \lambda)\mathbf{y}) \geq \lambda f(\mathbf{x}) + (1 - \lambda)f(\mathbf{y})
    \]
\end{definition}

We also define strictly convex and strictly concave functions by requiring the additional condition that the inequality in the definition is strict whenever \(\mathbf{x} \neq \mathbf{y}\) and \(\lambda \in (0,1)\).

\subsection{Properties of convex and concave functions}
\begin{enumerate}
    \item
          (Sum) If \(f\) and \(g\) are convex (concave), then so is \(f + g\) .
    \item
          (Multiplication by a scalar) If \(f\) is convex (concave) and
          \(\lambda \geq 0\) , then \(\lambda f\) is convex (concave).
    \item
          Composition of functions:
          \begin{itemize}
              \item
                    If \(f\) and \(g\) are convex and \(g\) is (weakly) increasing, then
                    \(g \circ f\) is convex.
              \item
                    If \(f\) and \(g\) are concave and \(g\) is (weakly) increasing, then
                    \(g \circ f\) is concave.
          \end{itemize}
    \item
          Maximum and minimum:
          \begin{itemize}
              \item
                    If \(f\) and \(g\) are convex, then \(\max (f, g)\) is convex.
              \item
                    If \(f\) and \(g\) are concave, then \(\min (f, g)\) is concave.
          \end{itemize}
\end{enumerate}

\subsection{Characterization for differentiable functions}

\begin{proposition}
    Let \(f: C \subset \mathbb{R}^{n} \rightarrow \mathbb{R}\), with \(C\) convex. Assume that \(f\) is differentiable at \(\mathbf{x}_{0} \in \mathbb{R}^{n}\). Then:
    \begin{align*}
        f \text{(strictly) convex} \iff \forall \mathbf{x} \in \mathbb{R}^{n}, f(\mathbf{x}) & \geq f(\mathbf{x_0}) + \nabla f(\mathbf{x_0}) \cdot (\mathbf{x} - \mathbf{x_0}) \\
                                                                                             & (>)
    \end{align*}
    or
    \begin{align*}
        f \text{(strictly) concave} \iff \forall \mathbf{x} \in \mathbb{R}^{n}, f(\mathbf{x}) & \leq f(\mathbf{x_0}) + \nabla f(\mathbf{x_0}) \cdot (\mathbf{x} - \mathbf{x_0}) \\
                                                                                              & (<)
    \end{align*}
\end{proposition}

\subsection{Characterization for \(C^2\) functions}

\begin{proposition}
    Let \(f:C\subset \mathbb{R}^{n}\longrightarrow \mathbb{R}\) , with \(C\) convex and \(f\in C^{2}\) . Then:

    i) \(f\) is \textbf{convex} \(\iff \mathbf{H}f(\mathbf{x})\) is \textbf{positive} semi-definite \(\forall \mathbf{x}\in \mathbb{R}^{n}\)

    ii) \(f\) is \textbf{concave} \(\iff \mathbf{H}f(\mathbf{x})\) is \textbf{negative} semi-definite \(\forall \mathbf{x}\in \mathbb{R}^{n}\)
\end{proposition}

\begin{proposition}
    Let \(f:C\subset \mathbb{R}^{n}\longrightarrow \mathbb{R}\) , with \(C\) convex and \(f\in C^{2}\). Then:

    i) If \(\mathbf{H}f(\mathbf{x})\) is \textbf{positive} definite \(\forall \mathbf{x}\in \mathbb{R}^{n} \Longrightarrow f\) is \textbf{strictly convex}.

    ii) If \(\mathbf{H}f(\mathbf{x})\) is \textbf{negative} definite \(\forall \mathbf{x}\in \mathbb{R}^{n} \Longrightarrow f\) is \textbf{strictly concave}.
\end{proposition}

\section{Quasi-concave and Quasi-convex Functions}

\begin{definition}[Upper-level Set]
    Given a point \(a \in \mathbb{R}\), the \textbf{upper-level set} of \(f\) at level \(a\) is the set: \(\left\{\mathbf{x}: f(\mathbf{x}) \geq a\right\}\).
\end{definition}


\begin{definition}[Lower-level Set]
    Given a point \(a \in \mathbb{R}\), the \textbf{lower-level set} of \(f\) at level \(a\) is the set: \(\left\{\mathbf{x}: f(\mathbf{x}) \leq a\right\}\).
\end{definition}


\begin{definition}[Quasi-concave]
    A function is \textbf{quasi-concave} if all its upper-level sets are convex.
\end{definition}

\begin{definition}[Quasi-convex]
    A function is \textbf{quasi-convex} if all its lower-level sets are convex.
\end{definition}

\textbf{Equivalent definitions:}

\begin{definition}[Quasi-concave]
    \(f:S\subseteq \mathbb{R}^{n}\longrightarrow \mathbb{R}\) (S convex set) is quasi-concave if
    \[
        \forall \mathbf{x},\mathbf{y}\in S,\forall \lambda \in [0,1],f(\lambda \mathbf{x} + (1 - \lambda)\mathbf{y})\geq \min \{f(\mathbf{x}),f(\mathbf{y})\}
    \]
\end{definition}

\begin{definition}[Quasi-convex]
    \(f:S\subseteq \mathbb{R}^{n}\longrightarrow \mathbb{R}\) (S convex set) is quasi-convex if
    \[
        \forall \mathbf{x},\mathbf{y}\in S,\forall \lambda \in [0,1],f(\lambda \mathbf{x} + (1 - \lambda)\mathbf{y})\leq \max \{f(\mathbf{x}),f(\mathbf{y})\}
    \]
\end{definition}

We also define \textbf{strictly quasi-concave (quasi-convex) functions} by requiring the additional condition that the inequality in the definition is strict whenever \(\mathbf{x}\neq \mathbf{y}\) and \(\lambda \in (0,1)\).

\begin{remark*}Examples of Quasi-convex and Quasi-concave:

    Quasi-convex: $f(x_1,x_2) = \max(x_1, x_2)$.

    Quasi-concave: $f(x_1,x_2) = \min(x_1, x_2)$.
\end{remark*}

\subsection{Properties of quasi-concave (quasi-convex) functions}

\begin{enumerate}
    \item
          If \(f\) is quasi-concave and \(g\) increasing, then \(g \circ f\) is
          quasi-concave.
    \item
          If \(f\) is quasi-convex and \(g\) increasing, then \(g \circ f\) is
          quasi-convex.
\end{enumerate}

